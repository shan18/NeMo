inference:
  greedy: false                                   # Whether or not to use sampling ; use greedy decoding otherwise
  top_k: 0                                        # The number of highest probability vocabulary tokens to keep for top-k-filtering.
  top_p: 0.9                                      # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
  temperature: 1.0                                # sampling temperature
  add_BOS: True                                   # add the bos token at the begining of the prompt
  tokens_to_generate: null                        # this will be filled based on scrolls task length
  all_probs: false                                # whether return the log prob for all the tokens in vocab
  repetition_penalty: 1.2                         # The parameter for repetition penalty. 1.0 means no penalty.
  min_tokens_to_generate: 0                       # The minimum length of the sequence to be generated.
  compute_logprob: false                          # a flag used to compute logprob of all the input text, a very special case of running inference, default False
  end_strings: ["<|endoftext|>", "<extra_id_1>"]  # generation will stop when one of these tokens is generated
  output_file: null                               # output file path
  task: null                                      # Zero-Scrolls task name
  data_dir: null                                  # Zero-Scrolls data directory
  batch_size: 1                                   # batch size for inference
  max_seq_length: null                            # max sequence length for inference, use model's pretraining seq length if not set
  n_jobs: -1                                      # number of jobs to process input data, -1 means using all the cores
  use_flash_attention: true                       # will override the model's config
  apply_query_key_layer_scaling: null             # will override the model's config
  remove_newline_tab: true

trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  logger: false                                   # logger provided by exp_manager
  precision: bf16                                 # 16, 32, or bf16

gpt_model_file: null                              # GPT nemo file path
checkpoint_dir: null                              # checkpoint file dir. This is used to load the PTL checkpoint generated during the GPT training
checkpoint_name: null                             # PTL checkpoint file name, only used for PTL checkpoint loading
hparams_file: null                                # model configuration file, only used for PTL checkpoint loading
tensor_model_parallel_size: -1
pipeline_model_parallel_size: -1
pipeline_model_parallel_split_rank: -1            # used for encoder and decoder model (0 for others)
megatron_amp_O2: true                             # use megatron-style mixed precision
enforce_fp32_pos_idx: false                       # enforce fp32 for calculating position embeddings
seq_len_interpolation_factor: null                # position interpolation factor
server: false                                     # whether launch the API server
port: 5555                                        # the port number for the inference server
web_server: false                                 # whether launch the web inference server
share: false                                      # whether create a public URL
username: test                                    # user name for web client
password: test2                                   # password for web client
web_port: 9889                                    # the port number of the web server
chat: false                                       # use the chat interface
chatbot_config:
  user: User
  assistant: Assistant
  prompt: "<extra_id_0>System\n\n<extra_id_1>User\n{context}\n<extra_id_1>Assistant\n"
